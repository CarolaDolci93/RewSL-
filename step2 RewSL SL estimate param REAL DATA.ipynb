{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2: ESTIMATE PARAMETERS OF REAL DATA\n",
    "\n",
    "a) use the same log_likelihood function to estimate the parametrs of each dataset we collected in the experiment. Here we also run the estimation process 10 times (with different initial guess) and we take the better esimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_realData(parameters, file_name):\n",
    "    learning_rate, intercept, slope = parameters\n",
    "    Prev_p = {'midLeft': 0.25, 'midRight': 0.25, 'downLeft': 0.25, 'downRight': 0.25}\n",
    "    outcomes = {}\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df[(df['epoch'] != 'practice') & (df['key_resp.rt'].notna())].copy()\n",
    "\n",
    "    log_likelihood = 0\n",
    "      \n",
    "    for i in range(len(df)):\n",
    "        current_targetPos = df.iloc[i]['targetPos']\n",
    "        current_resp = df.iloc[i]['key_resp.corr']\n",
    "        real_RT = df.iloc[i]['key_resp.rt']\n",
    "        if real_RT <= 0 or pd.isna(real_RT):\n",
    "            print(\"Not valid RT\")\n",
    "            continue  # salta valori non validi\n",
    "        real_RS = 1 / real_RT\n",
    "        \n",
    "        # Compute outcomes\n",
    "        for loc in Prev_p.keys():\n",
    "            if current_resp == 1:\n",
    "                if loc == current_targetPos:\n",
    "                    outcomes[loc] = 1\n",
    "                else:\n",
    "                    outcomes[loc] = 0\n",
    "            elif current_resp == 0:\n",
    "                outcomes[loc] = 0\n",
    "\n",
    "        #calculate RS\n",
    "        predicted_RS = intercept + slope * Prev_p[current_targetPos] \n",
    "        error = real_RS - predicted_RS\n",
    "        log_likelihood += -error ** 2 \n",
    "\n",
    "        # Update probabilities\n",
    "        if current_resp == 1:\n",
    "            for loc in Prev_p.keys():\n",
    "                Prev_p[loc] += learning_rate * (outcomes[loc] - Prev_p[loc])\n",
    "                #Prev_p[loc] = min(max(Prev_p[loc], 0.0001), 0.9999)     \n",
    "            total = sum(Prev_p.values())\n",
    "            for loc in Prev_p.keys():\n",
    "                Prev_p[loc] /= total\n",
    "        elif current_resp == 0:\n",
    "            continue \n",
    "            \n",
    "            \n",
    "    return -log_likelihood\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"G:\\\\RewSL EEG\\\\Analysis\\\\1 Behavioural\\\\SL\\\\\"\n",
    "file_pattern = os.path.join(data_dir, \"*_RewSL_SL_EEG_*.csv\")\n",
    "#List participants\n",
    "participant_files = glob.glob(file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation complete. Results saved in 'RealData_estimated_parameters.csv'.\n"
     ]
    }
   ],
   "source": [
    "#List where I can store the estimated parametrs\n",
    "participant_ids = []\n",
    "estimated_learning_rate = []\n",
    "estimated_intercept = []\n",
    "estimated_slope = []\n",
    "\n",
    "for file in participant_files:\n",
    "    data = pd.read_csv(file)\n",
    "    #extract participant's number\n",
    "    if \"participant\" in data.columns:\n",
    "        participant_id = data[\"participant\"].iloc[0]  \n",
    "    else:\n",
    "        print(f\"Skipping {file}: 'participant' column not found.\")\n",
    "        continue\n",
    "    \n",
    "    best_result = None\n",
    "    best_log_likelihood = float('inf')\n",
    "    #try the parameter estimation 10 times\n",
    "    for _ in range(5):  \n",
    "        random_init_params = [\n",
    "            np.random.uniform(0.1, 0.9),  #Learning rate\n",
    "            np.random.uniform(0.1, 10),   #Intercept\n",
    "            np.random.uniform(0.1, 10)       #Slope\n",
    "        ]\n",
    "    \n",
    "        result = minimize(log_likelihood_realData,random_init_params,args=(file,), bounds=[(0,1), (-100, 100), (-100, 100)])\n",
    "\n",
    "        if result.success and result.fun < best_log_likelihood:\n",
    "            best_log_likelihood = result.fun\n",
    "            best_result = result\n",
    "\n",
    "    #Store best results\n",
    "    participant_ids.append(participant_id)\n",
    "\n",
    "    if best_result:\n",
    "        estimated_learning_rate.append(best_result.x[0])\n",
    "        estimated_intercept.append(best_result.x[1])\n",
    "        estimated_slope.append(best_result.x[2])\n",
    "    else:\n",
    "        estimated_learning_rate.append(np.nan)\n",
    "        estimated_intercept.append(np.nan)\n",
    "        estimated_slope.append(np.nan)\n",
    "        print(f\"Optimization failed for participant {participant_id}\")\n",
    "\n",
    "#Creazte DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Participant_ID\": participant_ids,\n",
    "    \"Learning_Rate\": estimated_learning_rate,\n",
    "    \"Intercept\": estimated_intercept,\n",
    "    \"Slope\": estimated_slope\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"RealData_estimated_parameters.csv\", index=False)\n",
    "print(\"Estimation complete. Results saved in 'RealData_estimated_parameters.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: SIMULATE RT ON REAL DATASET BY USING THE PARAMETRS WE ESTIMATED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_RS(parameters, file_name): \n",
    "    learning_rate, intercept, slope = parameters\n",
    "    noise_std=0.1\n",
    "    Prev_p = {'midLeft': 0.25, 'midRight': 0.25, 'downLeft': 0.25, 'downRight': 0.25}\n",
    "    outcomes = {}\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df[(df['epoch'] != 'practice') & (df['key_resp.rt'].notna())].copy()\n",
    "    \n",
    "    # Lists to collect trial-wise values\n",
    "    simulated_RS = []\n",
    "    real_RS = []\n",
    "    prob_midLeft, prob_midRight, prob_downLeft, prob_downRight = [], [], [], []\n",
    "    outcome_midLeft, outcome_midRight, outcome_downLeft, outcome_downRight = [], [], [], []\n",
    "    prob_high, prob_lowA, prob_lowB, prob_lowC = [], [], [], []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        current_targetPos = df.iloc[i]['targetPos']\n",
    "        current_resp = df.iloc[i]['key_resp.corr']\n",
    "        real_RT = df.iloc[i]['key_resp.rt']\n",
    "        if real_RT <= 0:\n",
    "            real_RS_val = np.nan\n",
    "            print(\"Something wrong. RT = 0\")\n",
    "        else:\n",
    "            real_RS_val = 1 / real_RT\n",
    "        real_RS.append(real_RS_val)\n",
    "\n",
    "        #Compute outcomes\n",
    "        for loc in Prev_p.keys():\n",
    "            if current_resp == 1:\n",
    "                if loc == current_targetPos:\n",
    "                    outcomes[loc] = 1\n",
    "                else:\n",
    "                    outcomes[loc] = 0\n",
    "            elif current_resp == 0:\n",
    "                outcomes[loc] = 0\n",
    "\n",
    "        #Store current probabilities\n",
    "        prob_midLeft.append(Prev_p['midLeft'])\n",
    "        prob_midRight.append(Prev_p['midRight'])\n",
    "        prob_downLeft.append(Prev_p['downLeft'])\n",
    "        prob_downRight.append(Prev_p['downRight'])\n",
    "\n",
    "        #Store outcomes\n",
    "        outcome_midLeft.append(outcomes['midLeft'])\n",
    "        outcome_midRight.append(outcomes['midRight'])\n",
    "        outcome_downLeft.append(outcomes['downLeft'])\n",
    "        outcome_downRight.append(outcomes['downRight'])\n",
    "\n",
    "        #map probability for each target frequency\n",
    "        if df.iloc[i]['Slblock'] == 1:\n",
    "            prob_high_val = Prev_p['midLeft']\n",
    "            prob_lowA_val = Prev_p['midRight']\n",
    "            prob_lowB_val = Prev_p['downLeft']\n",
    "            prob_lowC_val = Prev_p['downRight'] \n",
    "        elif df.iloc[i]['Slblock'] == 2:\n",
    "            prob_high_val = Prev_p['midRight']\n",
    "            prob_lowA_val = Prev_p['midLeft']\n",
    "            prob_lowB_val = Prev_p['downRight']\n",
    "            prob_lowC_val = Prev_p['downLeft'] \n",
    "        elif df.iloc[i]['Slblock'] == 3:\n",
    "            prob_high_val = Prev_p['downLeft']\n",
    "            prob_lowA_val = Prev_p['downRight']\n",
    "            prob_lowB_val = Prev_p['midLeft']\n",
    "            prob_lowC_val = Prev_p['midRight']\n",
    "        elif df.iloc[i]['Slblock'] == 4:\n",
    "            prob_high_val = Prev_p['downRight']\n",
    "            prob_lowA_val = Prev_p['downLeft']\n",
    "            prob_lowB_val = Prev_p['midRight']\n",
    "            prob_lowC_val = Prev_p['midLeft']\n",
    "\n",
    "        #Store probabilities by frequency\n",
    "        prob_high.append(prob_high_val)\n",
    "        prob_lowA.append(prob_lowA_val)\n",
    "        prob_lowB.append(prob_lowB_val)\n",
    "        prob_lowC.append(prob_lowC_val)\n",
    "\n",
    "        #Predict RS \n",
    "        predict_RS = intercept + slope * Prev_p[current_targetPos] + np.random.normal(0, noise_std)\n",
    "        simulated_RS.append(predict_RS)\n",
    "\n",
    "        #Update probabilities\n",
    "        if current_resp == 1:\n",
    "            for loc in Prev_p.keys():\n",
    "                Prev_p[loc] += learning_rate * (outcomes[loc] - Prev_p[loc])\n",
    "                #Prev_p[loc] = min(max(Prev_p[loc], 0.0001), 0.9999)     \n",
    "            total = sum(Prev_p.values())\n",
    "            for loc in Prev_p.keys():\n",
    "                Prev_p[loc] /= total\n",
    "        elif current_resp == 0:\n",
    "            continue \n",
    "          \n",
    "\n",
    "    # Add columns to dataframe\n",
    "    df['simulated_RS'] = simulated_RS\n",
    "    df['real_RS'] = real_RS\n",
    "\n",
    "    df['prev_p_midLeft'] = prob_midLeft\n",
    "    df['prev_p_midRight'] = prob_midRight\n",
    "    df['prev_p_downLeft'] = prob_downLeft\n",
    "    df['prev_p_downRight'] = prob_downRight\n",
    "\n",
    "    df['outcome_midLeft'] = outcome_midLeft\n",
    "    df['outcome_midRight'] = outcome_midRight\n",
    "    df['outcome_downLeft'] = outcome_downLeft\n",
    "    df['outcome_downRight'] = outcome_downRight\n",
    "\n",
    "    df['prev_p_high'] = prob_high\n",
    "    df['prev_p_lowA'] = prob_lowA\n",
    "    df['prev_p_lowB'] = prob_lowB\n",
    "    df['prev_p_lowC'] = prob_lowC\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 10 - Learning Rate: 0.5119, Intercept: 1.6119, Slope: 0.1404\n",
      "Participant 11 - Learning Rate: 0.3701, Intercept: 1.2112, Slope: 0.2011\n",
      "Participant 12 - Learning Rate: 0.0010, Intercept: 1.1485, Slope: 2.2681\n",
      "Participant 13 - Learning Rate: 0.4208, Intercept: 1.2304, Slope: 0.2734\n",
      "Participant 14 - Learning Rate: 0.2544, Intercept: 1.2470, Slope: 0.2495\n",
      "Participant 15 - Learning Rate: 0.6876, Intercept: 1.7118, Slope: 0.4177\n",
      "Participant 16 - Learning Rate: 0.0162, Intercept: 1.2712, Slope: 0.3423\n",
      "Participant 17 - Learning Rate: 0.0000, Intercept: 0.5044, Slope: 3.3559\n",
      "Participant 18 - Learning Rate: 0.0000, Intercept: -23.5720, Slope: 100.0000\n",
      "Participant 19 - Learning Rate: 0.0000, Intercept: -23.7913, Slope: 100.0000\n",
      "Participant 1 - Learning Rate: 0.0002, Intercept: 2.8667, Slope: -3.9929\n",
      "Participant 20 - Learning Rate: 0.0000, Intercept: -20.9570, Slope: 89.4361\n",
      "Participant 21 - Learning Rate: 0.1197, Intercept: 1.4741, Slope: 0.3201\n",
      "Participant 22 - Learning Rate: 0.5305, Intercept: 1.5171, Slope: 0.2160\n",
      "Participant 23 - Learning Rate: 0.2038, Intercept: 1.4660, Slope: 0.1545\n",
      "Participant 24 - Learning Rate: 0.0000, Intercept: -23.2029, Slope: 100.0000\n",
      "Participant 25 - Learning Rate: 0.0119, Intercept: 1.2053, Slope: 0.7608\n",
      "Participant 26 - Learning Rate: 0.0002, Intercept: 27.7890, Slope: -100.0000\n",
      "Participant 27 - Learning Rate: 0.1940, Intercept: 1.2654, Slope: 0.3193\n",
      "Participant 28 - Learning Rate: 0.0000, Intercept: -23.5894, Slope: 100.0000\n",
      "Participant 29 - Learning Rate: 0.0001, Intercept: 0.0177, Slope: 5.6027\n",
      "Participant 2 - Learning Rate: 0.0012, Intercept: 1.2005, Slope: 1.3151\n",
      "Participant 30 - Learning Rate: 0.4712, Intercept: 1.0869, Slope: 0.2049\n",
      "Participant 3 - Learning Rate: 0.0000, Intercept: 26.5092, Slope: -100.0000\n",
      "Participant 4 - Learning Rate: 0.8062, Intercept: 2.0466, Slope: 0.2044\n",
      "Participant 5 - Learning Rate: 0.0002, Intercept: -0.9210, Slope: 8.0312\n",
      "Participant 6 - Learning Rate: 0.0000, Intercept: -23.3946, Slope: 100.0000\n",
      "Participant 7 - Learning Rate: 0.1308, Intercept: 1.7288, Slope: 0.1609\n",
      "Participant 8 - Learning Rate: 0.2286, Intercept: 1.5798, Slope: 0.2477\n",
      "Participant 9 - Learning Rate: 0.0000, Intercept: 10.9951, Slope: -37.8047\n"
     ]
    }
   ],
   "source": [
    "params_df = pd.read_csv(\"RealData_estimated_parameters.csv\")\n",
    "\n",
    "# Simulate for all participants\n",
    "all_simulated_data = []\n",
    "\n",
    "for file in participant_files:\n",
    "    data = pd.read_csv(file)\n",
    "    participant_id = data[\"participant\"].iloc[0]\n",
    "\n",
    "    # Get their parameters\n",
    "    row = params_df[params_df[\"Participant_ID\"] == participant_id]\n",
    "    if row.empty:\n",
    "        continue\n",
    "\n",
    "    #intercept = row[\"Intercept\"].values[0]\n",
    "    #slope = row[\"Slope\"].values[0]\n",
    "    #fixed_learning_rate = 0.3\n",
    "    #params = [fixed_learning_rate, intercept, slope]\n",
    "    params = row[[\"Learning_Rate\", \"Intercept\", \"Slope\"]].values.flatten()\n",
    "    print(f\"Participant {participant_id} - Learning Rate: {params[0]:.4f}, Intercept: {params[1]:.4f}, Slope: {params[2]:.4f}\")\n",
    "    simulated_df = simulate_RS(params, file)\n",
    "    simulated_df['Participant_ID'] = participant_id\n",
    "    all_simulated_data.append(simulated_df)\n",
    "\n",
    "# Concatenate all\n",
    "final_simulated_df = pd.concat(all_simulated_data, ignore_index=True)\n",
    "final_simulated_df.to_csv(\"simulated_RS_all_participants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df = pd.read_csv(\"simulated_RS_all_participants.csv\")\n",
    "participants = simulated_df['Participant_ID'].unique()\n",
    "\n",
    "# Loop through each participant\n",
    "for participant in participants:\n",
    "    subset = simulated_df[simulated_df['Participant_ID'] == participant].copy()\n",
    "    subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    sim_std = subset[\"simulated_RS\"].std()\n",
    "    real_std = subset[\"real_RS\"].std()\n",
    "\n",
    "    print(f\"Participant {participant} → Simulated RS std: {sim_std:.4f}, Real RS std: {real_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df = pd.read_csv(\"simulated_RS_all_participants.csv\")\n",
    "participants = simulated_df[\"Participant_ID\"].unique()\n",
    "\n",
    "for participant_id in participants:\n",
    "    subset = simulated_df[simulated_df[\"Participant_ID\"] == participant_id].reset_index(drop=True)\n",
    "    subset = subset.copy()\n",
    "    subset['bin'] = subset.index // 10\n",
    "\n",
    "    # Compute mean of each variable within each bin\n",
    "    binned = subset.groupby('bin')[[\"prev_p_high\", \"prev_p_lowA\", \"prev_p_lowB\", \"prev_p_lowC\"]].mean().reset_index()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(binned[\"prev_p_high\"], label=\"High\", color='red')\n",
    "    plt.plot(binned[\"prev_p_lowA\"], label=\"Low A\", color='blue')\n",
    "    plt.plot(binned[\"prev_p_lowB\"], label=\"Low B\", color='green')\n",
    "    plt.plot(binned[\"prev_p_lowC\"], label=\"Low C\", color='purple')\n",
    "\n",
    "    plt.title(f\"Prev_p Probabilities Over Time — Participant {participant_id}\")\n",
    "    plt.xlabel(\"Trial\")\n",
    "    plt.ylabel(\"Prev_p\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) plot the real vs simulated RT to see if they were well simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df = pd.read_csv(\"simulated_RS_all_participants.csv\")\n",
    "participants = simulated_df['Participant_ID'].unique()\n",
    "\n",
    "# Loop through each participant\n",
    "for participant in participants:\n",
    "    subset = simulated_df[simulated_df['Participant_ID'] == participant].copy()\n",
    "    subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(data=subset, x='real_RS', y='simulated_RS')\n",
    "\n",
    "    # Diagonal reference line (y = x)\n",
    "    min_val = min(subset['real_RS'].min(), subset['simulated_RS'].min())\n",
    "    max_val = max(subset['real_RS'].max(), subset['simulated_RS'].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')\n",
    "\n",
    "    plt.xlabel('Real RS')\n",
    "    plt.ylabel('Simulated RS')\n",
    "    plt.title(f'Participant {participant}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df = pd.read_csv(\"simulated_RS_all_participants.csv\")\n",
    "participants = simulated_df['Participant_ID'].unique()\n",
    "\n",
    "#Loop through each participant\n",
    "for participant in participants:\n",
    "    subset = simulated_df[simulated_df['Participant_ID'] == participant].copy()\n",
    "\n",
    "    #Group by trial number within block and compute means\n",
    "    grouped = subset.groupby('trials_e.thisN')[['real_RS', 'simulated_RS']].mean().reset_index()\n",
    "\n",
    "    #Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(data=grouped, x='real_RS', y='simulated_RS')\n",
    "\n",
    "    # Diagonal reference line (y = x)\n",
    "    min_val = min(grouped['real_RS'].min(), grouped['simulated_RS'].min())\n",
    "    max_val = max(grouped['real_RS'].max(), grouped['simulated_RS'].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')\n",
    "\n",
    "    plt.xlabel('Mean Real RS (per trial index)')\n",
    "    plt.ylabel('Mean Simulated RS (per trial index)')\n",
    "    plt.title(f'Participant {participant}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df = simulated_df.dropna(subset=['real_RS'])\n",
    "\n",
    "# Get unique participant IDs\n",
    "participants = simulated_df['Participant_ID'].unique()\n",
    "\n",
    "# Loop through each participant\n",
    "for participant in participants:\n",
    "    # Filter data for the current participant\n",
    "    subset = simulated_df[simulated_df['Participant_ID'] == participant]\n",
    "    \n",
    "    # Group by condition and compute mean RS values\n",
    "    mean_rts = subset.groupby('t_frequency').agg(\n",
    "        real_RS=('real_RS', 'mean'),\n",
    "        simulated_RS=('simulated_RS', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.lineplot(data=mean_rts, x='t_frequency', y='real_RS', label='Real RS', marker='o')\n",
    "    sns.lineplot(data=mean_rts, x='t_frequency', y='simulated_RS', label='Simulated RS', marker='s')\n",
    "    \n",
    "    plt.title(f'Participant {participant} - Mean RS by t_frequency')\n",
    "    plt.xlabel('t_frequency')\n",
    "    plt.ylabel('Mean RS')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 1: r = 0.04, p = 0.1896\n",
      "Participant 2: r = 0.02, p = 0.4960\n",
      "Participant 3: r = 0.04, p = 0.1794\n",
      "Participant 4: r = 0.17, p = 0.0000\n",
      "Participant 5: r = 0.07, p = 0.0269\n",
      "Participant 6: r = 0.17, p = 0.0000\n",
      "Participant 7: r = 0.04, p = 0.1470\n",
      "Participant 8: r = 0.08, p = 0.0067\n",
      "Participant 9: r = 0.01, p = 0.8587\n",
      "Participant 10: r = 0.07, p = 0.0130\n",
      "Participant 11: r = 0.15, p = 0.0000\n",
      "Participant 12: r = 0.10, p = 0.0007\n",
      "Participant 13: r = 0.22, p = 0.0000\n",
      "Participant 14: r = 0.11, p = 0.0002\n",
      "Participant 15: r = 0.25, p = 0.0000\n",
      "Participant 16: r = 0.07, p = 0.0187\n",
      "Participant 17: r = 0.02, p = 0.4984\n",
      "Participant 18: r = 0.18, p = 0.0000\n",
      "Participant 19: r = 0.09, p = 0.0025\n",
      "Participant 20: r = 0.07, p = 0.0234\n",
      "Participant 21: r = 0.12, p = 0.0000\n",
      "Participant 22: r = 0.14, p = 0.0000\n",
      "Participant 23: r = 0.06, p = 0.0545\n",
      "Participant 24: r = 0.16, p = 0.0000\n",
      "Participant 25: r = 0.14, p = 0.0000\n",
      "Participant 26: r = 0.07, p = 0.0208\n",
      "Participant 27: r = 0.12, p = 0.0000\n",
      "Participant 28: r = 0.08, p = 0.0044\n",
      "Participant 29: r = -0.03, p = 0.3129\n",
      "Participant 30: r = 0.14, p = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdolci\\AppData\\Local\\Temp\\ipykernel_19932\\1491741923.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  correlations = simulated_df.groupby('Participant_ID', group_keys=False).apply(compute_corr).reset_index()\n"
     ]
    }
   ],
   "source": [
    "simulated_df = pd.read_csv(\"simulated_RS_all_participants.csv\")\n",
    "participants = simulated_df['Participant_ID'].unique()\n",
    "\n",
    "def compute_corr(df):\n",
    "    r, p = pearsonr(df['real_RS'], df['simulated_RS'])\n",
    "    return pd.Series({'pearson_r': r, 'p_value': p})\n",
    "\n",
    "# Apply without including grouping column in the group data\n",
    "correlations = simulated_df.groupby('Participant_ID', group_keys=False).apply(compute_corr).reset_index()\n",
    "\n",
    "# Print results\n",
    "for _, row in correlations.iterrows():\n",
    "    print(f\"Participant {int(row['Participant_ID'])}: r = {row['pearson_r']:.2f}, p = {row['p_value']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
